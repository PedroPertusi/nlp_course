{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging\n",
    "\n",
    "Part-of-speech tagging is the task of identifying the grammatical classes of words in a sentence. In this class, we will use the concept of n-grams to help us automatically identify grammatical classes.\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "In the sentences below, identify the *nouns*, *verbs*, *adjectives*, and *adverbs*:\n",
    "\n",
    "1. Today I woke up serenely and saw that it was a beautiful and calm day.\n",
    "1. The mutation of fungi is capable of controlling people's minds!\n",
    "1. Every day, the morning Sun comes and challenges us!\n",
    "1. It is no use trying to make an automatic system that does something we do not understand the result of!\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "There are many words that always have the same PoS definition - maybe the word Sun, for example, is always a noun. However, there are others that can change their meaning according to the context, such as \"house\": \"I live in a house\" (noun), versus \"I like house music\" (adjective), versus \"The museums house a collection of ancient artifacts\" (verb).\n",
    "\n",
    "Because of that, it is important to use context to determine the\n",
    "\n",
    "Recall that our language model context model was:\n",
    "\n",
    "$$\n",
    "ùëÉ(ùë§_ùëõ‚à£ùë§_{ùëõ‚àí1}, w_{n-2}, \\cdots, w_{n-L})\n",
    "$$\n",
    "\n",
    "Now, we can make a small change and use:\n",
    "\n",
    "$$\n",
    "ùëÉ(\\text{tag}‚à£w_n, ùë§_{ùëõ‚àí1}, w_{n-2}, \\cdots, w_{n-L})\n",
    "$$\n",
    "\n",
    "Similarly to the language models, we can use a fallback n-gram strategy to make a reasonable model. But, first, we will need to download a corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/pedropertusi/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A corpus is a collection of texts. The [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus) has many phrases, with categories and word-level tags for part-of-speech. This was done manually by a team of brave taggers. Here are some highlights on how to use the [Brown corpus in NLTK](https://www.nltk.org/book/ch02.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a list of categories in the Brown corpus\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Too', 'often', 'a', 'beginning', 'bodybuilder', 'has', 'to', 'do', 'his', 'training', 'secretly', 'either', 'because', 'his', 'parents', \"don't\", 'want', 'sonny-boy', 'to', '``', 'lift', 'all', 'those', 'old', 'barbell', 'things', \"''\", 'because', '``', \"you'll\", 'stunt', 'your', 'growth', \"''\", 'or', 'because', 'childish', 'taunts', 'from', 'his', 'schoolmates', ',', 'like', '``', 'Hey', 'lookit', 'Mr.', 'America', ';', ';'], ['whaddya', 'gonna', 'do', 'with', 'all', 'those', 'muscles', '(', 'of', 'which', 'he', 'has', 'none', 'at', 'the', 'time', ')', \"''\", '?', '?'], ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a list of sentences in a category:\n",
    "brown.sents(categories='hobbies')\n",
    "# Each sentence is a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Too', 'QL'), ('often', 'RB'), ('a', 'AT'), ('beginning', 'VBG'), ('bodybuilder', 'NN'), ('has', 'HVZ'), ('to', 'TO'), ('do', 'DO'), ('his', 'PP$'), ('training', 'NN'), ('secretly', 'RB'), ('either', 'CC'), ('because', 'CS'), ('his', 'PP$'), ('parents', 'NNS'), (\"don't\", 'DO*'), ('want', 'VB'), ('sonny-boy', 'NN'), ('to', 'TO'), ('``', '``'), ('lift', 'VB'), ('all', 'ABN'), ('those', 'DTS'), ('old', 'JJ'), ('barbell', 'NN'), ('things', 'NNS'), (\"''\", \"''\"), ('because', 'CS'), ('``', '``'), (\"you'll\", 'PPSS+MD'), ('stunt', 'VB'), ('your', 'PP$'), ('growth', 'NN'), (\"''\", \"''\"), ('or', 'CC'), ('because', 'CS'), ('childish', 'JJ'), ('taunts', 'NNS'), ('from', 'IN'), ('his', 'PP$'), ('schoolmates', 'NNS'), (',', ','), ('like', 'CS'), ('``', '``'), ('Hey', 'UH'), ('lookit', 'VB+IN'), ('Mr.', 'NP'), ('America', 'NP'), (';', '.'), (';', '.')], [('whaddya', 'WDT+BER+PP'), ('gonna', 'VBG+TO'), ('do', 'DO'), ('with', 'IN'), ('all', 'ABN'), ('those', 'DTS'), ('muscles', 'NNS'), ('(', '('), ('of', 'IN'), ('which', 'WDT'), ('he', 'PPS'), ('has', 'HVZ'), ('none', 'PN'), ('at', 'IN'), ('the', 'AT'), ('time', 'NN'), (')', ')'), (\"''\", \"''\"), ('?', '.'), ('?', '.')], ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a list of *tagged* sentences in a category:\n",
    "brown.tagged_sents(categories='hobbies')\n",
    "# You can find the meaning of the tags by looking at the Wikipedia article: https://en.wikipedia.org/wiki/Brown_Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**\n",
    "\n",
    "Make code to count the proportion of each tag throughout the corpus. If you finish this too quickly, subdivide your count by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'QL': 721, 'RB': 2311, 'AT': 6946, 'VBG': 1509, 'NN': 12465, 'HVZ': 235, 'TO': 988, 'DO': 124, 'PP$': 993, 'CC': 2905, 'CS': 1378, 'NNS': 5123, 'DO*': 41, 'VB': 2966, '``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}\n",
      "{'QL': 0.008755844313558808, 'RB': 0.028064849110449937, 'AT': 0.0843524196976137, 'VBG': 0.018325338514785353, 'NN': 0.15137531119072195, 'HVZ': 0.0028538466209241603, 'TO': 0.01199829983605562, 'DO': 0.001505859493594025, 'PP$': 0.012059019976926346, 'CC': 0.03527840184589228, 'CS': 0.016734470823972312, 'NNS': 0.0622138563361467, 'DO*': 0.0004979051551399599, 'VB': 0.03601918756451515, '``': 0.0027566943955309978, 'ABN': 0.002841702592750015, 'DTS': 0.0025381018883963813, 'JJ': 0.059299289574351814, \"''\": 0.00316959135345194, 'PPSS+MD': 0.00044932904244337845, 'IN': 0.10432934604408282, ',': 0.046535915963325035, 'UH': 8.500819721901755e-05, 'VB+IN': 1.2144028174145364e-05, 'NP': 0.017645272937033215, '.': 0.052425769627785536, 'WDT+BER+PP': 1.2144028174145364e-05, 'VBG+TO': 1.2144028174145364e-05, '(': 0.0034489040014572835, 'WDT': 0.004213977776428441, 'PPS': 0.007237840791790637, 'PN': 0.0010079543384540653, ')': 0.0033881838605865564, 'NN+HVZ': 1.2144028174145364e-05, 'VBN+TO': 1.2144028174145364e-05, 'HV': 0.004566154593478657, 'AP': 0.009569494201226547, 'PPS+BEZ': 0.000534337239662396, 'PPO': 0.005938429777157083, 'WPS': 0.002902422733620742, 'HVN': 0.00018216042261218047, 'VBN': 0.02734835144817536, 'CD': 0.015993685105349445, 'BEN': 0.0013965632400267168, 'DT': 0.008124354848503249, 'BEZ': 0.011755419272572712, '--': 0.0035217681705021556, 'RP': 0.004177545691906005, 'JJR': 0.002562389944744672, 'PPSS': 0.0110024895257757, 'VBD': 0.00749286538344769, 'NN-TL': 0.011330378286477625, 'MD*': 0.000534337239662396, 'RBR': 0.0010929625356730828, 'FW-AT': 2.4288056348290727e-05, 'FW-NN': 0.0003278887607019248, 'VBZ': 0.008197219017548121, 'FW-JJ': 4.8576112696581454e-05, 'RBT': 0.00021859250713461656, 'DT+BEZ': 6.072014087072682e-05, 'NP-TL': 0.00316959135345194, 'JJ-TL': 0.0039103770720748075, 'DTI': 0.0023802295221324915, 'EX+BEZ': 0.00014572833808974438, 'JJT': 0.001238690873762827, 'WP$': 0.0001700163944380351, 'BEDZ': 0.0032181674661485213, 'QL-TL': 1.2144028174145364e-05, 'WRB': 0.0034853360859797193, 'MD': 0.012034731920578056, 'DOD': 0.0003036007043536341, 'NP$': 0.0011779707328921003, 'DOZ*': 9.715222539316291e-05, 'BEDZ*': 2.4288056348290727e-05, \"'\": 0.0003036007043536341, 'OD': 0.0017608840852510778, 'NPS': 0.000534337239662396, 'CD-TL': 0.0010443864229765013, 'ABX': 0.0005100491833141053, '*': 0.0027081182828344163, 'WQL': 0.00037646487339850626, 'BER': 0.006460622988645334, 'IN-TL': 0.0007407857186228673, 'NNS-TL': 0.0021859250713461655, 'PPSS+HV': 0.00018216042261218047, 'BED': 0.0012872669864594085, 'HVG': 0.0002064484789604712, 'WPO': 0.00010929625356730828, 'BE': 0.006351326735078025, 'NNS$': 0.00014572833808974438, 'NN$': 0.0010808185074989375, 'PPL': 0.0005829133523589775, 'PPLS': 0.0002307365353087619, 'JJS': 0.00037646487339850626, 'VBG-TL': 0.00014572833808974438, 'EX': 0.0013844192118525715, ':': 0.001432995324549153, 'CD-HL': 0.000267168619831198, '.-HL': 0.0007529297467970125, 'RB+BEZ': 4.8576112696581454e-05, 'ABL': 0.0002307365353087619, 'HVD': 0.0008622260003643209, 'BEZ*': 8.500819721901755e-05, 'QLP': 0.0002793126480053434, 'PP$$': 8.500819721901755e-05, 'BER*': 4.8576112696581454e-05, 'VBN-TL': 0.0003278887607019248, 'HV*': 2.4288056348290727e-05, 'QL-HL': 3.6432084522436094e-05, 'JJ-HL': 0.0005464812678365414, 'CC-HL': 0.0002307365353087619, 'AT-HL': 0.00038860890157265163, 'NN-HL': 0.002562389944744672, 'NR': 0.0012751229582852633, 'DOZ': 0.00042504098609508776, 'NNS$-TL': 0.00012144028174145364, 'BEG': 0.00044932904244337845, 'DTX': 9.715222539316291e-05, 'AP$': 1.2144028174145364e-05, 'NR$': 0.000133584309915599, 'NN$-TL': 0.000133584309915599, 'AT-TL': 0.000133584309915599, 'FW-NN-TL': 0.00015787236626388972, 'FW-IN-TL': 7.286416904487219e-05, 'FW-NN$-TL': 1.2144028174145364e-05, 'BEM': 6.072014087072682e-05, 'DOD*': 3.6432084522436094e-05, 'FW-JJ-TL': 7.286416904487219e-05, 'PN$': 1.2144028174145364e-05, 'FW-CC-TL': 2.4288056348290727e-05, 'FW-*-TL': 1.2144028174145364e-05, 'FW-RB-TL': 1.2144028174145364e-05, 'PPSS+BEM': 2.4288056348290727e-05, 'NP-HL': 0.000267168619831198, ',-HL': 0.0002064484789604712, 'VBZ-HL': 3.6432084522436094e-05, 'IN-HL': 0.0004736170987916692, 'NNS-HL': 0.0012872669864594085, 'TO-HL': 0.000400752929746797, 'VB-HL': 0.000534337239662396, 'OD-HL': 3.6432084522436094e-05, 'NN-TL-HL': 6.072014087072682e-05, 'NNS-TL-HL': 2.4288056348290727e-05, 'JJ-TL-HL': 1.2144028174145364e-05, 'AP-HL': 0.00012144028174145364, 'VBG-HL': 0.000400752929746797, '---HL': 6.072014087072682e-05, 'VBN-HL': 0.00010929625356730828, 'NR-TL': 0.00019430445078632582, 'PPSS+BER': 0.0002307365353087619, 'NN+BEZ': 1.2144028174145364e-05, 'CC-TL': 0.00029145667617948875, 'VB+PPO': 3.6432084522436094e-05, 'OD-TL': 0.000133584309915599, 'VB-TL': 4.8576112696581454e-05, 'RB-TL': 2.4288056348290727e-05, 'AP-TL': 1.2144028174145364e-05, 'UH-TL': 1.2144028174145364e-05, 'CD$': 2.4288056348290727e-05, 'NPS-TL': 3.6432084522436094e-05, 'NNS$-HL': 1.2144028174145364e-05, 'NP$-HL': 2.4288056348290727e-05, 'NR-TL-HL': 4.8576112696581454e-05, 'NRS': 1.2144028174145364e-05, 'NR-HL': 2.4288056348290727e-05, 'NP$-TL': 6.072014087072682e-05, 'FW-IN': 8.500819721901755e-05, 'FW-UH-NC': 1.2144028174145364e-05, ':-HL': 0.0008865140567126115, 'DT-HL': 2.4288056348290727e-05, 'RP-HL': 6.072014087072682e-05, 'RB-HL': 6.072014087072682e-05, 'DO*-HL': 3.6432084522436094e-05, 'CD-TL-HL': 1.2144028174145364e-05, 'MD-HL': 1.2144028174145364e-05, 'PPSS-TL': 1.2144028174145364e-05, 'WRB-HL': 0.00024288056348290729, 'PPO-HL': 2.4288056348290727e-05, 'PP$-HL': 4.8576112696581454e-05, 'BEZ-HL': 3.6432084522436094e-05, 'PPS-HL': 0.00019430445078632582, 'DTI-HL': 2.4288056348290727e-05, 'WDT-HL': 0.00024288056348290729, 'FW-CC': 1.2144028174145364e-05, 'FW-PP$': 1.2144028174145364e-05, 'FW-NP': 1.2144028174145364e-05, 'FW-PPL': 1.2144028174145364e-05, 'FW-IN+AT-TL': 2.4288056348290727e-05, 'FW-AT-TL': 2.4288056348290727e-05, 'FW-NNS-TL': 3.6432084522436094e-05, 'FW-AT+NN-TL': 1.2144028174145364e-05, 'RB+BEZ-HL': 1.2144028174145364e-05, 'NN$-HL': 0.00019430445078632582, 'DOZ-HL': 0.00018216042261218047, '(-HL': 1.2144028174145364e-05, ')-HL': 1.2144028174145364e-05, 'WQL-TL': 1.2144028174145364e-05, 'TO-TL': 1.2144028174145364e-05, 'JJR-HL': 6.072014087072682e-05, 'HVZ*': 1.2144028174145364e-05, 'WDT+BEZ': 1.2144028174145364e-05, 'CS-HL': 1.2144028174145364e-05, 'PPSS-HL': 2.4288056348290727e-05, 'JJS-HL': 1.2144028174145364e-05, 'PP$-TL': 1.2144028174145364e-05, 'BE-HL': 1.2144028174145364e-05, 'DTS-HL': 1.2144028174145364e-05, 'DO-HL': 1.2144028174145364e-05, 'FW-IN+NN-TL': 1.2144028174145364e-05}\n"
     ]
    }
   ],
   "source": [
    "# Solve the exercise here\n",
    "dict = {}\n",
    "for sentence in brown.tagged_sents(categories='hobbies'):\n",
    "    for tag in sentence:\n",
    "        if tag[1] not in dict:\n",
    "            dict[tag[1]] = 1\n",
    "        else:\n",
    "            dict[tag[1]] += 1\n",
    "\n",
    "# take the proportion of each tag\n",
    "total = sum(dict.values())\n",
    "dict_proportion = {}\n",
    "for key in dict:\n",
    "    dict_proportion[key] = dict[key] / total\n",
    "\n",
    "print(dict)\n",
    "print(dict_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Suppose we have no idea how to choose a PoS tag for any word. Our best guess is to pick one. For example, in the code below, we tag all words as qualifiers (QL). As we can see from the evaluation process, this is not a very accurate method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Too', 'QL'), ('often', 'QL'), ('a', 'QL'), ('beginning', 'QL'), ('bodybuilder', 'QL'), ('has', 'QL'), ('to', 'QL'), ('do', 'QL'), ('his', 'QL'), ('training', 'QL'), ('secretly', 'QL'), ('either', 'QL'), ('because', 'QL'), ('his', 'QL'), ('parents', 'QL'), (\"don't\", 'QL'), ('want', 'QL'), ('sonny-boy', 'QL'), ('to', 'QL'), ('``', 'QL'), ('lift', 'QL'), ('all', 'QL'), ('those', 'QL'), ('old', 'QL'), ('barbell', 'QL'), ('things', 'QL'), (\"''\", 'QL'), ('because', 'QL'), ('``', 'QL'), (\"you'll\", 'QL'), ('stunt', 'QL'), ('your', 'QL'), ('growth', 'QL'), (\"''\", 'QL'), ('or', 'QL'), ('because', 'QL'), ('childish', 'QL'), ('taunts', 'QL'), ('from', 'QL'), ('his', 'QL'), ('schoolmates', 'QL'), (',', 'QL'), ('like', 'QL'), ('``', 'QL'), ('Hey', 'QL'), ('lookit', 'QL'), ('Mr.', 'QL'), ('America', 'QL'), (';', 'QL'), (';', 'QL')]\n",
      "[('Too', 'QL'), ('often', 'RB'), ('a', 'AT'), ('beginning', 'VBG'), ('bodybuilder', 'NN'), ('has', 'HVZ'), ('to', 'TO'), ('do', 'DO'), ('his', 'PP$'), ('training', 'NN'), ('secretly', 'RB'), ('either', 'CC'), ('because', 'CS'), ('his', 'PP$'), ('parents', 'NNS'), (\"don't\", 'DO*'), ('want', 'VB'), ('sonny-boy', 'NN'), ('to', 'TO'), ('``', '``'), ('lift', 'VB'), ('all', 'ABN'), ('those', 'DTS'), ('old', 'JJ'), ('barbell', 'NN'), ('things', 'NNS'), (\"''\", \"''\"), ('because', 'CS'), ('``', '``'), (\"you'll\", 'PPSS+MD'), ('stunt', 'VB'), ('your', 'PP$'), ('growth', 'NN'), (\"''\", \"''\"), ('or', 'CC'), ('because', 'CS'), ('childish', 'JJ'), ('taunts', 'NNS'), ('from', 'IN'), ('his', 'PP$'), ('schoolmates', 'NNS'), (',', ','), ('like', 'CS'), ('``', '``'), ('Hey', 'UH'), ('lookit', 'VB+IN'), ('Mr.', 'NP'), ('America', 'NP'), (';', '.'), (';', '.')]\n",
      "Accuracy: 0.00752244245568347\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "default_tagger = DefaultTagger('QL')\n",
    "sentence = brown.sents(categories='hobbies')[0]\n",
    "sentence_tagged = default_tagger.tag(sentence)\n",
    "sentence_ground_truth = brown.tagged_sents(categories='hobbies')[0]\n",
    "print(sentence_tagged)\n",
    "print(sentence_ground_truth)\n",
    "accuracy = default_tagger.accuracy([sentence_ground_truth])\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Change the evaluation process above to calculate accuracy over all the 'editorial' category of the Brown corpus\n",
    "1. Change the default tag to the most common tag you have found in Exercise 2. What is your change in accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "NLTK also comes with Unigram taggers. These taggers use $L=0$ in the context, that is, they always tag a word in the same way.\n",
    "\n",
    "The Unigram taggers requires training data. In the example below, we use the tagged sentences from the \"hobbies\" category for that.\n",
    "\n",
    "Of course, there is a change that the word we want does not appear in the training data. In this case, what is the best alternative? Well, we can choose the most common tag in the dataset - which is what we had been doing with the default tagger. This strategy is called \"backoff\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "unigram_tagger = UnigramTagger(brown.tagged_sents(categories='hobbies'), backoff=default_tagger)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Unigram tagger in the 'hobbies' category. Then, test it in sentences of each category.\n",
    "\n",
    "1. What do you observe in the results?\n",
    "1. Does this phenomenon happen for any category chosen for training?\n",
    "1. Make an experiment to find out if this phenomenon is due to train and test being from the same category, of if it is due to they containing strictly the same sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "We can also use n-gram taggers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import NgramTagger\n",
    "bigram_tagger = NgramTagger(n=2, train=brown.tagged_sents(categories='hobbies'), backoff=unigram_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Evaluate the bigram tagger\n",
    "1. Make a function that receives a value of $n$ and a training set as parameters, and returns a PoS tagger with n-gram taggers for that $n$ with a successive backoff option that\n",
    "1. Make a figure showing how the accuracy increases in the Brown corpus when $n$ is increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "One measure of wordiness in a text is the lexical density. Lexical Density is a concept that comes from the idea that nouns and verbs convey meaning, and other words are only auxiliary. The Lexical Density is calculated for a sentence as the number of nouns and verbs, divided by the total number of words in the sentence.\n",
    "\n",
    "Make a function that receives a sentence (and possibly a PoS tagger) as inputs and returns the sentence's lexical density.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
